{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c058c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import obonet\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "from Bio import SeqIO\n",
    "import Bio.PDB\n",
    "import urllib.request\n",
    "import py3Dmol\n",
    "import pylab\n",
    "import pickle as pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GAE\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.nn.models import MLP\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.pool import SAGPooling\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7313bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    pdbfiles: str = \"/home/paul/BioHack/pdbind-refined-set/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1416df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hetatm(input_pdb_file, output_pdb_file):\n",
    "    # Open the input PDB file for reading and the output PDB file for writing\n",
    "    with open(input_pdb_file, 'r') as infile, open(output_pdb_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            # Check if the line starts with 'HETATM' (non-protein atoms)\n",
    "            if line.startswith('HETATM'):\n",
    "                continue  # Skip this line (HETATM record)\n",
    "            # Write all other lines to the output file\n",
    "            outfile.write(line)\n",
    "            \n",
    "def get_atom_types_from_sdf(sdf_file):\n",
    "    supplier = Chem.SDMolSupplier(sdf_file)\n",
    "    atom_types = set()\n",
    "\n",
    "    for mol in supplier:\n",
    "        if mol is not None:\n",
    "            atoms = mol.GetAtoms()\n",
    "            atom_types.update([atom.GetSymbol() for atom in atoms])\n",
    "\n",
    "    return sorted(list(atom_types))\n",
    "\n",
    "def get_atom_types_from_mol2_split(mol2_file):\n",
    "    atom_types = set()\n",
    "\n",
    "    with open(mol2_file, 'r') as mol2:\n",
    "        reading_atoms = False\n",
    "        for line in mol2:\n",
    "            if line.strip() == '@<TRIPOS>ATOM':\n",
    "                reading_atoms = True\n",
    "                continue\n",
    "            elif line.strip() == '@<TRIPOS>BOND':\n",
    "                break\n",
    "\n",
    "            if reading_atoms:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 5:\n",
    "                    atom_type = parts[5]\n",
    "                    atom_types.add(atom_type)\n",
    "    \n",
    "    atom_types_split = set()\n",
    "    for atom in atom_types:\n",
    "        atom_types_split.add(str(atom).split('.')[0])\n",
    "        \n",
    "\n",
    "    return sorted(list(atom_types_split))\n",
    "\n",
    "def get_atom_types_from_mol2(mol2_file):\n",
    "    atom_types = set()\n",
    "\n",
    "    with open(mol2_file, 'r') as mol2:\n",
    "        reading_atoms = False\n",
    "        for line in mol2:\n",
    "            if line.strip() == '@<TRIPOS>ATOM':\n",
    "                reading_atoms = True\n",
    "                continue\n",
    "            elif line.strip() == '@<TRIPOS>BOND':\n",
    "                break\n",
    "\n",
    "            if reading_atoms:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 5:\n",
    "                    atom_type = parts[5]\n",
    "                    atom_types.add(atom_type)\n",
    "\n",
    "    return sorted(list(atom_types))\n",
    "\n",
    "def get_atom_list_from_mol2_split(mol2_file):\n",
    "    atoms = []\n",
    "    with open(mol2_file, 'r') as mol2:\n",
    "        reading_atoms = False\n",
    "        for line in mol2:\n",
    "            if line.strip() == '@<TRIPOS>ATOM':\n",
    "                reading_atoms = True\n",
    "                continue\n",
    "            elif line.strip() == '@<TRIPOS>BOND':\n",
    "                break\n",
    "\n",
    "            if reading_atoms:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 5:\n",
    "                    atom_type = parts[5]\n",
    "                    atoms.append(atom_type)\n",
    "    \n",
    "    atom_list = []\n",
    "    for atom in atoms:\n",
    "        atom_list.append(str(atom).split('.')[0])\n",
    "        \n",
    "\n",
    "    return atom_list\n",
    "\n",
    "def get_atom_list_from_mol2(mol2_file):\n",
    "    atoms = []\n",
    "    with open(mol2_file, 'r') as mol2:\n",
    "        reading_atoms = False\n",
    "        for line in mol2:\n",
    "            if line.strip() == '@<TRIPOS>ATOM':\n",
    "                reading_atoms = True\n",
    "                continue\n",
    "            elif line.strip() == '@<TRIPOS>BOND':\n",
    "                break\n",
    "\n",
    "            if reading_atoms:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 5:\n",
    "                    atom_type = parts[5]\n",
    "                    atoms.append(atom_type)\n",
    "\n",
    "    return atoms\n",
    "\n",
    "def get_bond_types_from_mol2(mol2_file):\n",
    "    bond_types = set()\n",
    "\n",
    "    with open(mol2_file, 'r') as mol2:\n",
    "        reading_bonds = False\n",
    "        for line in mol2:\n",
    "            if line.strip() == '@<TRIPOS>BOND':\n",
    "                reading_bonds = True\n",
    "                continue\n",
    "            elif reading_bonds and line.strip().startswith('@<TRIPOS>'):\n",
    "                break\n",
    "\n",
    "            if reading_bonds:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 4:\n",
    "                    bond_type = parts[3]\n",
    "                    bond_types.add(bond_type)\n",
    "\n",
    "    return sorted(list(bond_types))\n",
    "\n",
    "def read_mol2_bonds(mol2_file):\n",
    "    bonds = []\n",
    "    bond_types = []\n",
    "\n",
    "    with open(mol2_file, 'r') as mol2:\n",
    "        reading_bonds = False\n",
    "        for line in mol2:\n",
    "            if line.strip() == '@<TRIPOS>BOND':\n",
    "                reading_bonds = True\n",
    "                continue\n",
    "            elif reading_bonds and line.strip().startswith('@<TRIPOS>'):\n",
    "                break\n",
    "\n",
    "            if reading_bonds:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 4:\n",
    "                    atom1_index = int(parts[1])\n",
    "                    atom2_index = int(parts[2])\n",
    "                    bond_type = parts[3]\n",
    "                    bonds.append((atom1_index, atom2_index))\n",
    "                    bond_types.append(bond_type)\n",
    "\n",
    "    return bonds, bond_types\n",
    "\n",
    "def calc_residue_dist(residue_one, residue_two) :\n",
    "    \"\"\"Returns the C-alpha distance between two residues\"\"\"\n",
    "    diff_vector  = residue_one[\"CA\"].coord - residue_two[\"CA\"].coord\n",
    "    return np.sqrt(np.sum(diff_vector * diff_vector))\n",
    "\n",
    "def calc_dist_matrix(chain_one, chain_two) :\n",
    "    \"\"\"Returns a matrix of C-alpha distances between two chains\"\"\"\n",
    "    answer = np.zeros((len(chain_one), len(chain_two)), float)\n",
    "    for row, residue_one in enumerate(chain_one) :\n",
    "        for col, residue_two in enumerate(chain_two) :\n",
    "            answer[row, col] = calc_residue_dist(residue_one, residue_two)\n",
    "    return answer\n",
    "\n",
    "def calc_contact_map(uniID,map_distance):\n",
    "    pdb_code = uniID\n",
    "    pdb_filename = uniID+\"_pocket_clean.pdb\"\n",
    "    structure = Bio.PDB.PDBParser(QUIET = True).get_structure(pdb_code, (CFG.pdbfiles +'/'+pdb_code+'/'+pdb_filename))\n",
    "    model = structure[0]\n",
    "    flag1 = 0\n",
    "    flag2 = 0\n",
    "    idx = 0\n",
    "    index = []\n",
    "    chain_info = []\n",
    "    \n",
    "    for chain1 in model:\n",
    "        for resi in chain1:\n",
    "            index.append(idx)\n",
    "            idx += 1\n",
    "            chain_info.append([chain1.id,resi.id])\n",
    "        for chain2 in model:\n",
    "            if flag1 == 0:\n",
    "                dist_matrix = calc_dist_matrix(model[chain1.id], model[chain2.id])\n",
    "            else:\n",
    "                new_matrix = calc_dist_matrix(model[chain1.id], model[chain2.id])\n",
    "                dist_matrix = np.hstack((dist_matrix,new_matrix))\n",
    "            flag1 += 1\n",
    "        flag1 = 0\n",
    "        if flag2 == 0:\n",
    "            top_matrix = dist_matrix\n",
    "        else:\n",
    "            top_matrix = np.vstack((top_matrix,dist_matrix))\n",
    "        flag2 += 1\n",
    "    \n",
    "    contact_map = top_matrix < map_distance\n",
    "    return contact_map, index, chain_info\n",
    "\n",
    "\n",
    "def one_hot_encode_single_res(res):\n",
    "    allowed = set(\"GAVCPLIMWFKRHSTYNQDEUO\")\n",
    "    if not set(res).issubset(allowed):\n",
    "        invalid = set(res) - allowed\n",
    "        raise ValueError(f\"Sequence has broken AA: {invalid}\")\n",
    "        \n",
    "    AA_dict = {'GLY':torch.Tensor([1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "               'ALA':torch.Tensor([0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "               'VAL':torch.Tensor([0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "               'CYS':torch.Tensor([0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "               'PRO':torch.Tensor([0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "               'LEU':torch.Tensor([0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "               'ILE':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "               'MET':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "               'TRP':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "               'PHE':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "               'LYS':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "               'ARG':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "               'HIS':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "               'SER':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "               'THR':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "               'TYR':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0]),\n",
    "               'ASN':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0]),\n",
    "               'GLN':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0]),\n",
    "               'ASP':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0]),\n",
    "               'GLU':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0]),}\n",
    "    return AA_dict[res]\n",
    "\n",
    "AA_dictionary = {'GLY':torch.Tensor([1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "            'ALA':torch.Tensor([0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "            'VAL':torch.Tensor([0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "            'CYS':torch.Tensor([0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "            'PRO':torch.Tensor([0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "            'LEU':torch.Tensor([0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "            'ILE':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "            'MET':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "            'TRP':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "            'PHE':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "            'LYS':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "            'ARG':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "            'HIS':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "            'SER':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "            'THR':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0]),\n",
    "            'TYR':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0]),\n",
    "            'ASN':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0]),\n",
    "            'GLN':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0]),\n",
    "            'ASP':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0]),\n",
    "            'GLU':torch.Tensor([0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0]),}\n",
    "\n",
    "def uniID2graph(uniID,map_distance):\n",
    "    atom_name = 'CA'\n",
    "    node_feature = []\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    contact_map, index, chain_info = calc_contact_map(uniID,map_distance)\n",
    "    pdb_code = uniID\n",
    "    pdb_filename = uniID+\"_pocket_clean.pdb\"\n",
    "    structure = Bio.PDB.PDBParser(QUIET = True).get_structure(pdb_code, (CFG.pdbfiles +'/'+pdb_code+'/'+pdb_filename))\n",
    "    model = structure[0]\n",
    "    \n",
    "    for i in index:\n",
    "        node_feature.append(one_hot_encode_single_res(model[chain_info[i][0]][chain_info[i][1]].get_resname()))\n",
    "        for j in index:\n",
    "            if contact_map[i,j] == 1:\n",
    "                edge_index.append([i,j])\n",
    "                diff_vector = model[chain_info[i][0]][chain_info[i][1]]['CA'].coord - model[chain_info[j][0]][chain_info[j][1]]['CA'].coord\n",
    "                edge_feature = (np.sqrt(np.sum(diff_vector * diff_vector))/map_distance)        \n",
    "                edge_attr.append(edge_feature)\n",
    "                            \n",
    "    edge_index = np.array(edge_index)\n",
    "    edge_index = edge_index.transpose()\n",
    "    edge_index = torch.Tensor(edge_index)\n",
    "    edge_index = edge_index.to(torch.int64)\n",
    "    edge_attr = torch.Tensor(edge_attr)\n",
    "    node_feature = torch.stack(node_feature)\n",
    "    graph = Data(x = node_feature, edge_index = edge_index,edge_attr = edge_attr)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6058874",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bond_type_dict.pkl', 'rb') as f:\n",
    "    bond_type_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e3854aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Embeddings = []\n",
    "for i in bond_type_dict:\n",
    "    Embeddings.append(torch.Tensor(bond_type_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d357a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_tensor = torch.stack(Embeddings)\n",
    "dataset = TensorDataset(functions_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5fb750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, encoded_size, num_layers=1, num_heads=1, hidden_size=10):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_embedding = nn.Linear(input_size, hidden_size)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.encoder_fc = nn.Linear(hidden_size, encoded_size)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_fc = nn.Linear(encoded_size, hidden_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=hidden_size, nhead=num_heads)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.decoder_output = nn.Linear(hidden_size, input_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        x = self.encoder_embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        encoded = self.encoder_fc(x)\n",
    "        \n",
    "        # Decoding\n",
    "        x = self.decoder_fc(encoded)\n",
    "        x = self.transformer_decoder(x,encoded)\n",
    "        decoded = self.decoder_output(x)\n",
    "        \n",
    "        return decoded\n",
    "    \n",
    "    def encode(self,x):\n",
    "        x = self.encoder_embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        encoded = self.encoder_fc(x)\n",
    "        \n",
    "        return encoded\n",
    "    \n",
    "    def dencode(self,encoded):\n",
    "        x = self.decoder_fc(encoded)\n",
    "        x = self.transformer_decoder(x,encoded)\n",
    "        decoded = self.decoder_output(x)\n",
    "        \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b0d84a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] Loss: 0.7679 Val Loss: 0.7589\n",
      "Epoch [2/1000] Loss: 0.7549 Val Loss: 0.7478\n",
      "Epoch [3/1000] Loss: 0.7439 Val Loss: 0.7373\n",
      "Epoch [4/1000] Loss: 0.7340 Val Loss: 0.7271\n",
      "Epoch [5/1000] Loss: 0.7236 Val Loss: 0.7175\n",
      "Epoch [6/1000] Loss: 0.7143 Val Loss: 0.7081\n",
      "Epoch [7/1000] Loss: 0.7051 Val Loss: 0.6992\n",
      "Epoch [8/1000] Loss: 0.6963 Val Loss: 0.6907\n",
      "Epoch [9/1000] Loss: 0.6879 Val Loss: 0.6824\n",
      "Epoch [10/1000] Loss: 0.6795 Val Loss: 0.6745\n",
      "Epoch [11/1000] Loss: 0.6715 Val Loss: 0.6669\n",
      "Epoch [12/1000] Loss: 0.6643 Val Loss: 0.6595\n",
      "Epoch [13/1000] Loss: 0.6572 Val Loss: 0.6525\n",
      "Epoch [14/1000] Loss: 0.6502 Val Loss: 0.6458\n",
      "Epoch [15/1000] Loss: 0.6437 Val Loss: 0.6392\n",
      "Epoch [16/1000] Loss: 0.6372 Val Loss: 0.6330\n",
      "Epoch [17/1000] Loss: 0.6309 Val Loss: 0.6271\n",
      "Epoch [18/1000] Loss: 0.6253 Val Loss: 0.6211\n",
      "Epoch [19/1000] Loss: 0.6194 Val Loss: 0.6151\n",
      "Epoch [20/1000] Loss: 0.6135 Val Loss: 0.6092\n",
      "Epoch [21/1000] Loss: 0.6075 Val Loss: 0.6033\n",
      "Epoch [22/1000] Loss: 0.6013 Val Loss: 0.5975\n",
      "Epoch [23/1000] Loss: 0.5952 Val Loss: 0.5916\n",
      "Epoch [24/1000] Loss: 0.5897 Val Loss: 0.5850\n",
      "Epoch [25/1000] Loss: 0.5829 Val Loss: 0.5786\n",
      "Epoch [26/1000] Loss: 0.5762 Val Loss: 0.5722\n",
      "Epoch [27/1000] Loss: 0.5695 Val Loss: 0.5658\n",
      "Epoch [28/1000] Loss: 0.5637 Val Loss: 0.5590\n",
      "Epoch [29/1000] Loss: 0.5570 Val Loss: 0.5527\n",
      "Epoch [30/1000] Loss: 0.5509 Val Loss: 0.5467\n",
      "Epoch [31/1000] Loss: 0.5444 Val Loss: 0.5413\n",
      "Epoch [32/1000] Loss: 0.5403 Val Loss: 0.5359\n",
      "Epoch [33/1000] Loss: 0.5342 Val Loss: 0.5314\n",
      "Epoch [34/1000] Loss: 0.5299 Val Loss: 0.5273\n",
      "Epoch [35/1000] Loss: 0.5262 Val Loss: 0.5235\n",
      "Epoch [36/1000] Loss: 0.5229 Val Loss: 0.5201\n",
      "Epoch [37/1000] Loss: 0.5201 Val Loss: 0.5172\n",
      "Epoch [38/1000] Loss: 0.5168 Val Loss: 0.5150\n",
      "Epoch [39/1000] Loss: 0.5149 Val Loss: 0.5128\n",
      "Epoch [40/1000] Loss: 0.5130 Val Loss: 0.5111\n",
      "Epoch [41/1000] Loss: 0.5111 Val Loss: 0.5097\n",
      "Epoch [42/1000] Loss: 0.5101 Val Loss: 0.5084\n",
      "Epoch [43/1000] Loss: 0.5090 Val Loss: 0.5073\n",
      "Epoch [44/1000] Loss: 0.5082 Val Loss: 0.5064\n",
      "Epoch [45/1000] Loss: 0.5072 Val Loss: 0.5056\n",
      "Epoch [46/1000] Loss: 0.5062 Val Loss: 0.5050\n",
      "Epoch [47/1000] Loss: 0.5058 Val Loss: 0.5045\n",
      "Epoch [48/1000] Loss: 0.5050 Val Loss: 0.5040\n",
      "Epoch [49/1000] Loss: 0.5047 Val Loss: 0.5036\n",
      "Epoch [50/1000] Loss: 0.5043 Val Loss: 0.5033\n",
      "Epoch [51/1000] Loss: 0.5040 Val Loss: 0.5030\n",
      "Epoch [52/1000] Loss: 0.5038 Val Loss: 0.5027\n",
      "Epoch [53/1000] Loss: 0.5033 Val Loss: 0.5025\n",
      "Epoch [54/1000] Loss: 0.5030 Val Loss: 0.5023\n",
      "Epoch [55/1000] Loss: 0.5032 Val Loss: 0.5021\n",
      "Epoch [56/1000] Loss: 0.5029 Val Loss: 0.5019\n",
      "Epoch [57/1000] Loss: 0.5027 Val Loss: 0.5018\n",
      "Epoch [58/1000] Loss: 0.5024 Val Loss: 0.5017\n",
      "Epoch [59/1000] Loss: 0.5024 Val Loss: 0.5016\n",
      "Epoch [60/1000] Loss: 0.5022 Val Loss: 0.5015\n",
      "Epoch [61/1000] Loss: 0.5021 Val Loss: 0.5013\n",
      "Epoch [62/1000] Loss: 0.5020 Val Loss: 0.5013\n",
      "Epoch [63/1000] Loss: 0.5020 Val Loss: 0.5012\n",
      "Epoch [64/1000] Loss: 0.5019 Val Loss: 0.5011\n",
      "Epoch [65/1000] Loss: 0.5020 Val Loss: 0.5010\n",
      "Epoch [66/1000] Loss: 0.5016 Val Loss: 0.5010\n",
      "Epoch [67/1000] Loss: 0.5018 Val Loss: 0.5009\n",
      "Epoch [68/1000] Loss: 0.5016 Val Loss: 0.5009\n",
      "Epoch [69/1000] Loss: 0.5016 Val Loss: 0.5008\n",
      "Epoch [70/1000] Loss: 0.5016 Val Loss: 0.5008\n",
      "Epoch [71/1000] Loss: 0.5015 Val Loss: 0.5008\n",
      "Epoch [72/1000] Loss: 0.5015 Val Loss: 0.5008\n",
      "Epoch [73/1000] Loss: 0.5013 Val Loss: 0.5008\n",
      "Epoch [74/1000] Loss: 0.5015 Val Loss: 0.5007\n",
      "Epoch [75/1000] Loss: 0.5015 Val Loss: 0.5007\n",
      "Epoch [76/1000] Loss: 0.5013 Val Loss: 0.5007\n",
      "Epoch [77/1000] Loss: 0.5013 Val Loss: 0.5007\n",
      "Epoch [78/1000] Loss: 0.5015 Val Loss: 0.5006\n",
      "Epoch [79/1000] Loss: 0.5015 Val Loss: 0.5006\n",
      "Epoch [80/1000] Loss: 0.5014 Val Loss: 0.5006\n",
      "Epoch [81/1000] Loss: 0.5013 Val Loss: 0.5006\n",
      "Epoch [82/1000] Loss: 0.5015 Val Loss: 0.5006\n",
      "Epoch [83/1000] Loss: 0.5013 Val Loss: 0.5005\n",
      "Epoch [84/1000] Loss: 0.5012 Val Loss: 0.5005\n",
      "Epoch [85/1000] Loss: 0.5012 Val Loss: 0.5006\n",
      "Epoch [86/1000] Loss: 0.5014 Val Loss: 0.5005\n",
      "Epoch [87/1000] Loss: 0.5012 Val Loss: 0.5005\n",
      "Epoch [88/1000] Loss: 0.5012 Val Loss: 0.5005\n",
      "Epoch [89/1000] Loss: 0.5014 Val Loss: 0.5005\n",
      "Epoch [90/1000] Loss: 0.5013 Val Loss: 0.5005\n",
      "Epoch [91/1000] Loss: 0.5012 Val Loss: 0.5005\n",
      "Epoch [92/1000] Loss: 0.5013 Val Loss: 0.5005\n",
      "Epoch [93/1000] Loss: 0.5013 Val Loss: 0.5005\n",
      "Epoch [94/1000] Loss: 0.5013 Val Loss: 0.5005\n",
      "Epoch [95/1000] Loss: 0.5012 Val Loss: 0.5005\n",
      "Epoch [96/1000] Loss: 0.5011 Val Loss: 0.5005\n",
      "Epoch [97/1000] Loss: 0.5012 Val Loss: 0.5005\n",
      "Epoch [98/1000] Loss: 0.5012 Val Loss: 0.5005\n",
      "Epoch [99/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [100/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [101/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [102/1000] Loss: 0.5011 Val Loss: 0.5005\n",
      "Epoch [103/1000] Loss: 0.5014 Val Loss: 0.5004\n",
      "Epoch [104/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [105/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [106/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [107/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [108/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [109/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [110/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [111/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [112/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [113/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [114/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [115/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [116/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [117/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [118/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [119/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [120/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [121/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [122/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [123/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [124/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [125/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [126/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [127/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [128/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [129/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [130/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [131/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [132/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [133/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [134/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [135/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [136/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [137/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [138/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [139/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [140/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [141/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [142/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [143/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [144/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [145/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [146/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [147/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [148/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [149/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [150/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [151/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [152/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [153/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [154/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [155/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [156/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [157/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [158/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [159/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [160/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [161/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [162/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [163/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [164/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [165/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [166/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [167/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [168/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [169/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [170/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [171/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [172/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [173/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [174/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [175/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [176/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [177/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [178/1000] Loss: 0.5010 Val Loss: 0.5004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [179/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [180/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [181/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [182/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [183/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [184/1000] Loss: 0.5014 Val Loss: 0.5004\n",
      "Epoch [185/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [186/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [187/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [188/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [189/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [190/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [191/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [192/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [193/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [194/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [195/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [196/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [197/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [198/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [199/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [200/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [201/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [202/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [203/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [204/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [205/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [206/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [207/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [208/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [209/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [210/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [211/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [212/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [213/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [214/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [215/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [216/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [217/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [218/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [219/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [220/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [221/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [222/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [223/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [224/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [225/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [226/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [227/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [228/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [229/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [230/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [231/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [232/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [233/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [234/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [235/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [236/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [237/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [238/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [239/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [240/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [241/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [242/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [243/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [244/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [245/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [246/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [247/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [248/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [249/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [250/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [251/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [252/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [253/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [254/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [255/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [256/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [257/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [258/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [259/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [260/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [261/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [262/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [263/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [264/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [265/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [266/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [267/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [268/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [269/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [270/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [271/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [272/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [273/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [274/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [275/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [276/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [277/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [278/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [279/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [280/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [281/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [282/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [283/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [284/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [285/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [286/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [287/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [288/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [289/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [290/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [291/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [292/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [293/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [294/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [295/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [296/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [297/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [298/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [299/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [300/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [301/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [302/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [303/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [304/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [305/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [306/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [307/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [308/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [309/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [310/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [311/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [312/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [313/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [314/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [315/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [316/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [317/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [318/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [319/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [320/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [321/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [322/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [323/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [324/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [325/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [326/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [327/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [328/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [329/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [330/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [331/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [332/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [333/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [334/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [335/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [336/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [337/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [338/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [339/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [340/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [341/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [342/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [343/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [344/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [345/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [346/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [347/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [348/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [349/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [350/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [351/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [352/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [353/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [354/1000] Loss: 0.5012 Val Loss: 0.5004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [355/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [356/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [357/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [358/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [359/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [360/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [361/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [362/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [363/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [364/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [365/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [366/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [367/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [368/1000] Loss: 0.5014 Val Loss: 0.5004\n",
      "Epoch [369/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [370/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [371/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [372/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [373/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [374/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [375/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [376/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [377/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [378/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [379/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [380/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [381/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [382/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [383/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [384/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [385/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [386/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [387/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [388/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [389/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [390/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [391/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [392/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [393/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [394/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [395/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [396/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [397/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [398/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [399/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [400/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [401/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [402/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [403/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [404/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [405/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [406/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [407/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [408/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [409/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [410/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [411/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [412/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [413/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [414/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [415/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [416/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [417/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [418/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [419/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [420/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [421/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [422/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [423/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [424/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [425/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [426/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [427/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [428/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [429/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [430/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [431/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [432/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [433/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [434/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [435/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [436/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [437/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [438/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [439/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [440/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [441/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [442/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [443/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [444/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [445/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [446/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [447/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [448/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [449/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [450/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [451/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [452/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [453/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [454/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [455/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [456/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [457/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [458/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [459/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [460/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [461/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [462/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [463/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [464/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [465/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [466/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [467/1000] Loss: 0.5014 Val Loss: 0.5004\n",
      "Epoch [468/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [469/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [470/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [471/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [472/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [473/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [474/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [475/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [476/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [477/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [478/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [479/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [480/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [481/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [482/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [483/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [484/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [485/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [486/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [487/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [488/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [489/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [490/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [491/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [492/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [493/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [494/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [495/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [496/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [497/1000] Loss: 0.5014 Val Loss: 0.5004\n",
      "Epoch [498/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [499/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [500/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [501/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [502/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [503/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [504/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [505/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [506/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [507/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [508/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [509/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [510/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [511/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [512/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [513/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [514/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [515/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [516/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [517/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [518/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [519/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [520/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [521/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [522/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [523/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [524/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [525/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [526/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [527/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [528/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [529/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [530/1000] Loss: 0.5011 Val Loss: 0.5004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [531/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [532/1000] Loss: 0.5014 Val Loss: 0.5004\n",
      "Epoch [533/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [534/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [535/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [536/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [537/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [538/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [539/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [540/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [541/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [542/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [543/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [544/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [545/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [546/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [547/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [548/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [549/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [550/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [551/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [552/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [553/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [554/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [555/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [556/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [557/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [558/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [559/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [560/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [561/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [562/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [563/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [564/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [565/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [566/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [567/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [568/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [569/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [570/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [571/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [572/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [573/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [574/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [575/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [576/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [577/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [578/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [579/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [580/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [581/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [582/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [583/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [584/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [585/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [586/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [587/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [588/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [589/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [590/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [591/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [592/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [593/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [594/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [595/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [596/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [597/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [598/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [599/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [600/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [601/1000] Loss: 0.5014 Val Loss: 0.5004\n",
      "Epoch [602/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [603/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [604/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [605/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [606/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [607/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [608/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [609/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [610/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [611/1000] Loss: 0.5014 Val Loss: 0.5004\n",
      "Epoch [612/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [613/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [614/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [615/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [616/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [617/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [618/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [619/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [620/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [621/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [622/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [623/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [624/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [625/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [626/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [627/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [628/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [629/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [630/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [631/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [632/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [633/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [634/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [635/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [636/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [637/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [638/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [639/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [640/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [641/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [642/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [643/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [644/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [645/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [646/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [647/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [648/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [649/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [650/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [651/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [652/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [653/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [654/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [655/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [656/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [657/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [658/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [659/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [660/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [661/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [662/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [663/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [664/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [665/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [666/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [667/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [668/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [669/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [670/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [671/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [672/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [673/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [674/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [675/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [676/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [677/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [678/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [679/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [680/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [681/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [682/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [683/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [684/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [685/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [686/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [687/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [688/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [689/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [690/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [691/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [692/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [693/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [694/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [695/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [696/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [697/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [698/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [699/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [700/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [701/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [702/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [703/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [704/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [705/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [706/1000] Loss: 0.5011 Val Loss: 0.5004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [707/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [708/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [709/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [710/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [711/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [712/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [713/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [714/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [715/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [716/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [717/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [718/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [719/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [720/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [721/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [722/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [723/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [724/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [725/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [726/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [727/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [728/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [729/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [730/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [731/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [732/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [733/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [734/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [735/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [736/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [737/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [738/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [739/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [740/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [741/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [742/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [743/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [744/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [745/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [746/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [747/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [748/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [749/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [750/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [751/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [752/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [753/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [754/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [755/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [756/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [757/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [758/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [759/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [760/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [761/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [762/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [763/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [764/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [765/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [766/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [767/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [768/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [769/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [770/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [771/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [772/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [773/1000] Loss: 0.5014 Val Loss: 0.5004\n",
      "Epoch [774/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [775/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [776/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [777/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [778/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [779/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [780/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [781/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [782/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [783/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [784/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [785/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [786/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [787/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [788/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [789/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [790/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [791/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [792/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [793/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [794/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [795/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [796/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [797/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [798/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [799/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [800/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [801/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [802/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [803/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [804/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [805/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [806/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [807/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [808/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [809/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [810/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [811/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [812/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [813/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [814/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [815/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [816/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [817/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [818/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [819/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [820/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [821/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [822/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [823/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [824/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [825/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [826/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [827/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [828/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [829/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [830/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [831/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [832/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [833/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [834/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [835/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [836/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [837/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [838/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [839/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [840/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [841/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [842/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [843/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [844/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [845/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [846/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [847/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [848/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [849/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [850/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [851/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [852/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [853/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [854/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [855/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [856/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [857/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [858/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [859/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [860/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [861/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [862/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [863/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [864/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [865/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [866/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [867/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [868/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [869/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [870/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [871/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [872/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [873/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [874/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [875/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [876/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [877/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [878/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [879/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [880/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [881/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [882/1000] Loss: 0.5010 Val Loss: 0.5004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [883/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [884/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [885/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [886/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [887/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [888/1000] Loss: 0.5014 Val Loss: 0.5004\n",
      "Epoch [889/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [890/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [891/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [892/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [893/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [894/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [895/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [896/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [897/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [898/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [899/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [900/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [901/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [902/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [903/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [904/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [905/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [906/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [907/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [908/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [909/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [910/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [911/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [912/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [913/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [914/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [915/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [916/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [917/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [918/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [919/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [920/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [921/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [922/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [923/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [924/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [925/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [926/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [927/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [928/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [929/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [930/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [931/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [932/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [933/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [934/1000] Loss: 0.5009 Val Loss: 0.5004\n",
      "Epoch [935/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [936/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [937/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [938/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [939/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [940/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [941/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [942/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [943/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [944/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [945/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [946/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [947/1000] Loss: 0.5012 Val Loss: 0.5004\n",
      "Epoch [948/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [949/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [950/1000] Loss: 0.5010 Val Loss: 0.5004\n",
      "Epoch [951/1000] Loss: 0.5013 Val Loss: 0.5004\n",
      "Epoch [952/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [953/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [954/1000] Loss: 0.5011 Val Loss: 0.5004\n",
      "Epoch [955/1000] Loss: 0.5011 Val Loss: 0.5004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m autoencoder(inputs)\n\u001b[1;32m     48\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, inputs)\n\u001b[0;32m---> 50\u001b[0m         inputs\u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Print the average loss for this epoch\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder = Autoencoder(input_size = 5, encoded_size=1, num_layers=6,hidden_size = 1)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "autoencoder = autoencoder.to(DEVICE)\n",
    "\n",
    "# Define a loss function (e.g., Mean Squared Error) and an optimizer (e.g., Adam)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,500], gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000  # Adjust the number of epochs as needed\n",
    "lowest = 0.0015\n",
    "losses = []\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0].to(DEVICE).float()\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = autoencoder(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, inputs)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        inputs= inputs.to('cpu')\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    for batch in dataloader_val:\n",
    "        with torch.no_grad():\n",
    "            inputs = batch[0].to(DEVICE).float()\n",
    "        \n",
    "            outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "        \n",
    "            inputs= inputs.to('cpu')\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    # Print the average loss for this epoch\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_val_loss = val_loss / len(dataloader_val)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f} Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    if avg_loss < 0.0015 and epoch > 750:\n",
    "        if lowest > avg_loss:\n",
    "            torch.save(autoencoder,('bond_autoencoder_11012023_'+str(epoch+1)))\n",
    "            lowest = avg_loss\n",
    "            epoch_num = epoch+1\n",
    "    \n",
    "    #if epoch > 100:\n",
    "        #torch.save(autoencoder,('autoencoder_98var_10062023_'+str(epoch)))\n",
    "\n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "254900c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = torch.load('AA_atom_autoencoder_11012023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a098681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "534b34cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "func = random.choice(Embeddings).unsqueeze(0).to(DEVICE)\n",
    "pred = sigmoid(autoencoder(func.float()))\n",
    "\n",
    "\n",
    "func = random.choice(Embeddings).unsqueeze(0).to(DEVICE)\n",
    "pred = sigmoid(autoencoder(func.float()))\n",
    "\n",
    "x_axis = np.linspace(1,5,5)\n",
    "\n",
    "plt.bar(x_axis,func.squeeze().to('cpu').numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.bar(x_axis,pred.squeeze().to('cpu').detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08c094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
